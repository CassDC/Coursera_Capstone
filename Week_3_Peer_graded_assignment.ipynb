{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Segmenting and clustering neighborhoods in Toronto </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1 - Scraping a Wikipedia page</h2>\n",
    "\n",
    "<p>In Canada, postal codes beginning with \"M\" are located within the city of Toronto in the province of Ontario. We will scrape the Wikipedia page <a href= \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\">List of postal codes of Canada: M</a> for data in the table of postal codes. We will create a <i>pandas</i> dataframe of this data.</p>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>This dataframe will consist of three columns, namely <b>PostalCode</b>, <b>Borough</b>, and <b>Neighborhood</b>.</li>\n",
    "    <li>The rows containing \"Not assigned\" in the borough field will be ignored.</li>\n",
    "    <li>When more than one neighborhood exists for a postal code area, the neighborhoods should be listed together in the <b>Neighborhood</b> column.</li>\n",
    "    <li>If a row contains an assigned borough, but no assigned neighborhood, the neighborhood should be the same as the borough.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Downloading required dependencies</h3>\n",
    "<p>This will install all libraries we will use for Part 1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library for vectorized computation\n",
    "import pandas as pd # library to process data as dataframes\n",
    "!pip install beautifulsoup4 #Beautiful Soup 4\n",
    "!pip install lxml #lmxl parser\n",
    "!pip install request #request library\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Creating the raw dataframe</h3>\n",
    "<p>Although Beautiful Soup was attempted to solve this problem, it did not work to iterate through the table. The pandas <i>read_html</i> method was used instead.</p>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>\n",
    "    Using the pandas <i>read_html</i> method and the url of the Wikipedia page, a list of dataframes is created. These dataframes represent the tables in the html file. As the first table is the one of interest, list[0] will return the dataframe we would like to work with.</li>\n",
    "    <li>The column heading <b>Postal code</b> is renamed to <b>PostalCode</b>.</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dataframe from a list\n",
    "list_ = pd.read_html('https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M',header=0)\n",
    "\n",
    "#first value of the list\n",
    "df1 = list_[0]\n",
    "\n",
    "#Postal code is renamed to PostalCode\n",
    "df1.rename(columns={\"Postal code\": \"PostalCode\"},inplace = True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Assigning blank neighborhood cells to borough cell values</h3>\n",
    "<p>It is necessary to assign any blank <b>Neighborhood </b>cell values to the value of the <b>Borough</b> cell in the same row.</p>\n",
    "<ul>\n",
    "    <li>Firstly, the NaN values in the table are changed to a blank string value in a new dataframe, df2.</li>\n",
    "    <li>Secondly, the <b>Neighborhood</b> cells containing a blank string are replaced with the <b>Borough</b> cell value in the same row using a function. This function iterates along the length of the index of the df2.</li>\n",
    "    </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing NaN cells to blank cells\n",
    "df2 = df1.fillna(\" \")\n",
    "\n",
    "#Assigning blank neighbourhoods to borough names\n",
    "for i in range(len(df2)):\n",
    "    if df2.loc[i,'Neighborhood'] == \" \":\n",
    "        df2.loc[i,'Neighborhood'] = df2.loc[i,'Borough']\n",
    "    else:\n",
    "        pass\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Removing rows with unassigned boroughs</h3>\n",
    "<p>\n",
    "    Now that there are no blank <b>Neighbourhood</b> cells, the entries in the table with unassigned <b>Borough</b> cells must be removed. An unintended consequence of step 3 was that rows with blank <b>Neighborhood</b> values and unassigned <b>Borough</b> values now have \"Not assigned\" as both the <b>Borough</b> and <b>Neighborhood</b> value. Luckily, this can be fixed in the next step of the data clean up.\n",
    "</p>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>First we create a new dataframe, df3, and we populate it with only the rows in df2 that <u>do not</u> contain \"Not assigned\" in the <b>Borough</b> field.</li>\n",
    "        <li>The index values from df2 remain and a new index for df3 must be created. A new dataframe, df4, is created and assigned as df3 with the index reset and the old index dropped.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows where borough = \"Not assigned\"\n",
    "df3 = df2[df2['Borough']!=\"Not assigned\"]\n",
    "\n",
    "#reset index\n",
    "df4 = df3.reset_index(drop = True)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Changing forward slashes to commas in the neighborhoods field</h3>\n",
    "<p>Now that this is an almost completely cleaned dataframe, some finishing touches must be added. Some postal codes contain multiple neighborhoods, which are listed in the dataframe and separated with forward slashes. The forward slashes in the <b>Neighborhood</b> field must now be changed to commas.</p>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>First we create a new dataframe, df5, a copy of df4.</li>\n",
    "        <li>Next, using a function iterating along the length of the index of df4, the <b>Neighborhood</b> field cells are reassigned to be the <b>Neighborhood</b> cells in df4 with the forward slash replaced with a comma. This is acheived using the .replace() method.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4\n",
    "\n",
    "\n",
    "#Replacing / with ,\n",
    "for i in range(len(df4)):\n",
    "    df5.loc[i,'Neighborhood'] = df4.loc[i,'Neighborhood'].replace(\" /\",\",\")\n",
    "    \n",
    "df5.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 6: Counting the number of rows</h3>\n",
    "<p>The number of rows in the dataframe, df5, can be counted using the .shape method and requesting the value of the first index, 0.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = df5.shape\n",
    "print(\"Number of rows =\",shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Obtaining geographical coordinates for postal codes</h2>\n",
    "<p>Now that the dataframe contains the postal code of each neighborhood in Toronto, as well as the borough and neighborhood name, we would like the geographical coordinates for each postal code. This will allow us to make use of Foursquare location data to cluster the neighborhoods based on venues in the area.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Downloading required CSV file</h3>\n",
    "<p>This will import the CSV file of latitudes and longitudes for each postal code.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latlong = pd.read_csv(\"https://cocl.us/Geospatial_data\")\n",
    "df_latlong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Sorting the main dataframe by PostalCode</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>The main dataframe, df5, is sorted by <b>PostalCode</b> with inplace = True to retain the values in this order.</li>\n",
    "        <li>A new dataframe, df6, is copied from df5. The index of df6 is the reset index of df5.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort df5 dataframe\n",
    "df5.sort_values(by = \"PostalCode\", inplace = True)\n",
    "\n",
    "#reset index\n",
    "df6 = df5.reset_index(drop = True)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Sorting the coordinated dataframe by Postal Code</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>The df_latlong dataframe is sorted by <b>Postal Code</b> with inplace = True to retain the values in this order.</li>\n",
    "        <li>A new dataframe, df_latlong2, is copied from df_latlong. The index of df_latlong2 is the reset index of df_latlong.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort df_latlong dataframe\n",
    "df_latlong.sort_values(by = \"Postal Code\",inplace = True)\n",
    "\n",
    "#reset index\n",
    "df_latlong2 = df_latlong.reset_index(drop = True)\n",
    "df_latlong2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Merging the two dataframes</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>First, <b>Postal Code</b> in the df_latlong2 dataframe is renamed to <b>PostalCode</b>. This is done because, to use the pandas.merge() function, the column names are used as keys and must be identical.</li>\n",
    "        <li>Next, the pandas.merge() function is used to merge the two dataframes based on <b>PostalCode</b> as a key.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First rename Postal Code to PostalCode to use as same key\n",
    "df_latlong2.rename(columns={\"Postal Code\": \"PostalCode\"},inplace = True)\n",
    "#Merging df6 and df_latlong3 on PostalCode as key\n",
    "df_data = pd.merge(df6,df_latlong2,on=\"PostalCode\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Exploring and clustering neighbourhoods in Toronto</h2>\n",
    "<p>We would like to explore and cluster the neighborhoods of Toronto. This will be done by using the Foursquare API to find the most popular venues in each neighborhood. These venues will be categorised on type and ranked by the ten most common venue types in a neighborhood. This data will then be used with <i>k-means</i> analysis to cluster the neighborhoods into 5 groups.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Downloading required dependencies</h3>\n",
    "<p>This will install all libraries we will use for Part 3.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Checking the number of boroughs and neighborhoods</h3>\n",
    "<p>We would like to find out the number of boroughs and neighborhoods in Toronto in our dataset.\n",
    "    <ul>\n",
    "        <li>The .unique() function is passsed over the <b>Borough</b> column. The length of this list is calculated to find the number of boroughs in our dataset. </li>\n",
    "        <li>The same set of functions is passed over the <b>Neighborhood</b> column to find the number of neighborhoods in the dataset. A caveat of this is that the number of unique lists of neighborhoods in the dataset is calculated, as each postal code can be associated with multiple neighborhoods.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many boroughs and neighbourhoods are there?\n",
    "print(\"The dataframe has {} boroughs and {} lists of neighbourhoods.\".format(\n",
    "    len(df_data[\"Borough\"].unique()),\n",
    "    len(df_data[\"Neighborhood\"].unique())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Finding the latitude and longitude of Toronto for a map</h3>\n",
    "    <p>To generate a map of Toronto using the Folium package, we need the geographical co-ordinates for the centre of the city. We use the GeoPy package to request the latitude and longitude of Toronto.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use geopy library to get coordinates of Toronto for a map\n",
    "address = \"Toronto, Ontario\"\n",
    "geolocator = Nominatim(user_agent = \"toronto_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print(\"The geographical coordinates of Toronto are {}, {}.\".format(latitude,longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Create a map of Toronto and the locations of postal codes</h3>\n",
    "    <p>We would like to generate a map centred on Toronto that shows the location of the postal codes of the city.\n",
    "    <ul>\n",
    "        <li>Using the Folium package, we centre a map on Toronto using the latitude and longitude we previously generated in step 3. We set the zoom_start level to 10.</li>\n",
    "        <li>Using the latitude and longitude of the postal codes, fetched from a CSV file in part 2 of the notebook, we generate markers on the Folium map of the locations of Toronto postal codes. By clicking on the markers, you can see the name of the neighborhood(s) associated with the postal code.</li>\n",
    "        </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a map of Toronto with postal codes\n",
    "map_toronto = folium.Map(location = [latitude,longitude], zoom_start = 10)\n",
    "\n",
    "for lat, lng, borough, neighborhood in zip(df_data[\"Latitude\"],df_data[\"Longitude\"],df_data[\"Borough\"],df_data[\"Neighborhood\"]):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html = True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius = 3,\n",
    "        popup = label,\n",
    "        color = \"#8B0000\",\n",
    "        fill = True,\n",
    "        fill_color = \"#DC143C\",\n",
    "        foll_opacity = 0.7,\n",
    "        parse_html = False).add_to(map_toronto)\n",
    "map_toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Defining Foursquare credentials and version</h3>\n",
    "    <p>As we will be using the Foursquare Places API to generate a list of the most popular venues in a neighborhood and the type of the venue, we will need to input our Foursquare credentials for the GET explore request.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = '0OYWUO1KIOBY4TMTOIAHFQMQPXKWH05S52HMPMZ5E2YB2FV4'\n",
    "CLIENT_SECRET = 'YVZW53FC1D2AKGOBVXBMKKSXKDH4NEUOAI0JBAUIVTSTNEJY'\n",
    "VERSION = '20180605'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 6: Testing Foursquare GET request for popular venues</h3>\n",
    "<p>\n",
    "    We will test requesting popular venues in a neighborhood using the Foursquare Places API. We will use the first entry in our dataset, Malvern and Rouge in M1B, to do this.\n",
    "    <ul>\n",
    "        <li>First we define the latitude and longitude of the neighborhood from our dataframe. This is then printed out.</li>\n",
    "        <li>We define the radius for our Foursqare GET explore request as 500m and the limit of venues to fetch as 100 venues.</li>\n",
    "        <li>We define the url of the explore GET request as the variable url.</li>\n",
    "        <li>We send the GET request and print out the json file results.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test latitude and longitude of postal code - Step 6, part 1\n",
    "neighborhood_latitude = df_data.loc[0,\"Latitude\"]\n",
    "neighborhood_longitude = df_data.loc[0,\"Longitude\"]\n",
    "print(\"For ({}), Latitude = {}, Longitude = {}\".format(df_data.loc[0,\"Neighborhood\"],neighborhood_latitude,neighborhood_longitude))\n",
    "\n",
    "#url to get top 10 venues in a postal code within a radius of 500m\n",
    "radius = 500\n",
    "LIMIT = 100\n",
    "\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    neighborhood_latitude, \n",
    "    neighborhood_longitude, \n",
    "    radius, \n",
    "    LIMIT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test url on test postal code - Step 6, part 2\n",
    "results = requests.get(url).json()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 7: Extracting information from the json file and structuring a dataframe</h3>\n",
    "<p><ul>\n",
    "    <li>We know that the information we are looking for is in the <i>items</i> key of the json file. We will use a function <b>get_category_type</b>, to extract the category of the venue. </li>\n",
    "    <li>We clean the json file and structure it into a pandas dataframe.</li>\n",
    "    </ul></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the category of a venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']\n",
    "    \n",
    "#Clean json and structure it into a dataframe\n",
    "venues = results[\"response\"][\"groups\"][0][\"items\"]\n",
    "\n",
    "#flatten json\n",
    "nearby_venues = pd.json_normalize(venues)\n",
    "nearby_venues\n",
    "#filter columns\n",
    "filtered_columns = ['venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "nearby_venues = nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "#filter the category for each row\n",
    "nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "#clean columns\n",
    "nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "nearby_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 8: Repeat the get_category_type function for all neighborhoods in the dataset</h3>\n",
    "<p>First we define the function, <b>getNearbyVenues</b> that uses the latitude, longitude and a radius to find the top 100 venues for a given neighborhood.\n",
    "    <ul>\n",
    "        <li>We make use of the same Foursquare GET request we used previously. We also extract the venues information from the resulting json file and create a dataframe containing this information.</li>\n",
    "        <li>We then run the function on the neighborhoods in our dataset. This returns the top 100 venues for each neighbourhood as well as their venue category.</li>\n",
    "        </ul>\n",
    "        </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to repeat process for all postal codes in Toronto - Step 8, part 1\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the above function on each postal code and create a new dataframe called toronto_venues - Step 8, part 2\n",
    "toronto_venues = getNearbyVenues(names = df_data[\"Neighborhood\"], latitudes = df_data[\"Latitude\"], longitudes = df_data[\"Longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 9: How large is the resulting dataframe?</h3>\n",
    "<p>We use the .shape function to see that the resulting dataframe size is 2151 rows and 7 columns. We display the first 5 rows of the dataframe, toronto_venues, here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toronto_venues.shape)\n",
    "toronto_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 10: One hot encoding of venues types for each neighborhood</h3>\n",
    "<p>In order to analyse and cluster the neighborhoods by most popular venue types, we one hot encode the data. This assigns a binary variable (0 or 1) for each unique venue category.\n",
    "    <ul>\n",
    "        <li>We apply one hot encoding using the pandas .get_dummies() function on the toronto_venues dataframe. This creates the toronto_onehot dataframe.</li>\n",
    "        <li>We then add the <b>Neighborhood</b> column back to the dataframe using the .insert() function.</li>\n",
    "        <li>Unfortunately, one of the venue types is \"neighborhood\", which will wreak havoc on the rest of our analysis. We will therefore remove the \"neighborhood\" venues type from the toronto_onehot dataframe.</li>\n",
    "        </ul>\n",
    "        </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "toronto_onehot = pd.get_dummies(toronto_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "#add neighborhood column back to dataframe\n",
    "toronto_onehot.insert(0, \"Neighborhood\", toronto_venues['Neighborhood'] , True) \n",
    "\n",
    "#Remove \"Neighborhood\" venue type\n",
    "toronto_onehot = toronto_onehot.loc[:,~toronto_onehot.columns.duplicated()]\n",
    "toronto_onehot.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 11: Group rows by neighborhood and calculate frequency of venue category occurance</h3>\n",
    "<p>We group the toronto_onehot dataset by <b>Neighborhood</b> and calculate the mean of the frequency of each venue category's occurence. We create a new dataframe, toronto_grouped, from this.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_grouped = toronto_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "toronto_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step 12: Sort venues in descending order</h3>\n",
    "<p>We sort the venues categories in each neighborhood to top 10 venue types. We then create a new dataframe, neighborhoods_venues_sorted, from this. This dataframe contains the 10 most common venue types in each neighborhood, listed by name in each column. This is printed out at the end.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to sort the venues in descending order\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]\n",
    "\n",
    "#Create new dataframe to display top ten venues for each neighborhood\n",
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe neighborhoods_venues_sorted\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] =toronto_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(toronto_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(toronto_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 13: Cluster the neighborhoods into 5 clusters using k-means analysis</h3>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>First we define 5 clusters to begin with.</li>\n",
    "        <li>Next, we drop the <b>Neighborhood</b> column from the toronto_grouped dataframe. We create a new dataframe, toronto_grouped_clustering, from this.</li>\n",
    "        <li>We run a Kmeans() clustering function on the toronto_grouped_clustering dataframe.</li>\n",
    "        <li>We create a new dataframe, neighborhoods_venues_sorted, that inserts the kmeans labels as <b>Cluster Labels</b>. This dataframe contains the clusters as well as the top 10 venue categories for each neighborhood.</li>\n",
    "        <li>We display this new dataframe.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run k-means to cluster the neighborhood into 5 clusters\n",
    "kclusters = 5\n",
    "\n",
    "toronto_grouped_clustering = toronto_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(toronto_grouped_clustering)\n",
    "\n",
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "#Show neighborhoods_venues_sorted\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 14: Merge the df_data and neighborhoods_venues_sorted dataframes</h3>\n",
    "<p>The df_data and neighborhoods_venues_sorted dataframes are merged into the toronto_merged dataframe. This dataframe will be used to show the clustering on a map of Toronto. We remove all the rows that contain NaN as the cluster label and venue categories. These are as a result of some neighborhoods in the original dataframe not returning venues in the GET request. A sample of the resulting dataframe is printed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_merged = df_data\n",
    "\n",
    "#merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "toronto_merged = toronto_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "\n",
    "#Drop NaN cluster labels - some neighborhoods didn't have venues\n",
    "toronto_merged = toronto_merged.dropna(subset = [\"Cluster Labels\"], axis = 0,inplace = False)\n",
    "toronto_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 15: Visualing the neighborhood clusters</h3>\n",
    "<p>We visualise the clusters as different coloured markers on a map of Toronto.\n",
    "    <ul>\n",
    "        <li>We create a Folium map centred on Toronto with a zoom_start of 10.</li>\n",
    "        <li>We set the colour scheme for the clusters as a range of 5 colours in colour_array. This uses the rainbow() function of the cm library.</li>\n",
    "        <li>We cast the <b>Cluster Labels</b> column type to integer so that the color assignment for our markers will work. This requires an integer or slice input.</li>\n",
    "        <li>We add markers onto the map defined from the cluster label, the name of the neighborhood and its associated latitude and longitude.</li>\n",
    "        <li>The clusters are grouped according to 5 colours. The clusters were determined based on the most common venue type in the neighborhoods.</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualise the cluster\n",
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "toronto_merged = toronto_merged.astype({\"Cluster Labels\": int})\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(toronto_merged['Latitude'], toronto_merged['Longitude'], toronto_merged['Neighborhood'], toronto_merged['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(Optional) Step 16: Examining the clusters</h3>\n",
    "<p>Optionally, we can examine each cluster and determine the discriminating venue categories that lead to the resulting clustering. Change the cluster_number variable to examine each cluster.</p>\n",
    "<p>From cluster 2, we can see the most popular venue types were playgrounds, parks and yoga studios.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 2\n",
    "toronto_merged.loc[toronto_merged['Cluster Labels'] == cluster_number, toronto_merged.columns[[1] + list(range(5, toronto_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
